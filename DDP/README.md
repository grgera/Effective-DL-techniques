# Distributed Data-Parallel
This repository contains an example implementation of DDP in PyTorch framework. For comparison several experiments have provided: 
   - `single GPU training`
   - `DDP with 2 GPU`
   - `DDP with 2 GPU + mixed-precision training`.

All experiments based on CIFAR10 data and PyTorch [official tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). Test part of dataset include 10k examles.

-----------
## Single GPU
```
python -m train_single.py
```
Accuracy: **27 %** \
Total elapsed time: **69.03** seconds \
Train 1 epoch: **13.08** seconds


## DDP 2 GPU
```
python -m torch.distributed.launch --nproc_per_node=2 train_ddp.py
```
Accuracy: **19 %** \
Total elapsed time: **97.03** seconds \
Train 1 epoch: **9.79** seconds


## DDP 2 GPU + mixed precision
```
python -m torch.distributed.launch --nproc_per_node=2 train_ddp_mixed.py
```
Accuracy: **15 %** \
Total elapsed time: **70.61** seconds \
Train 1 epoch: **6.52** seconds

---------
These results were close to the idea of N workers would give a speedup of N. A very important note here is that DistributedDataParallel uses an effective batch size of 4*256=1024 so it makes fewer model updates. Thatâ€™s why it scores a much lower validation accuracy (15% compared to 27% in the baseline).